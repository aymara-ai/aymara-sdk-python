{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AymaraAI Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Creating an eval with AymaraSDK\n",
    "- Fetching eval prompts\n",
    "- Calling OpenAI with those prompts\n",
    "- Creating an eval run with the responses\n",
    "\n",
    "## Requirements\n",
    "- Set `OPENAI_API_KEY` and `AYMARA_AI_API_KEY` in your environment or `.env` file.\n",
    "- Install dependencies: `pip install openai aymara-ai dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and imports\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "\n",
    "from aymara_ai import AymaraAI\n",
    "from aymara_ai.types.evals import (\n",
    "    EvalPrompt,\n",
    "    EvalResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not set in environment.\")\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the AymaraSDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AymaraAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eval(ai_description='Minimal SDK Example Eval', eval_type='safety', name='minimal-example-eval', ai_instructions='Answer the prompts as best as you can.', created_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 196000, tzinfo=TzInfo(UTC)), eval_instructions=None, eval_uuid='test.75853c8f-c590-4a83-9dba-f35c940b6a84', is_jailbreak=False, is_sandbox=False, language='en', modality='text', num_prompts=50, prompt_examples=None, status='created', updated_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 197000, tzinfo=TzInfo(UTC)), workspace_uuid=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_obj = client.evals.create(\n",
    "    ai_description=\"Minimal SDK Example Eval\",\n",
    "    ai_instructions=\"Answer the prompts as best as you can.\",\n",
    "    eval_type=\"safety\",\n",
    "    name=\"minimal-example-eval\",\n",
    "    num_prompts=5,\n",
    ")\n",
    "eval_id = eval_obj.eval_uuid\n",
    "if not eval_id:\n",
    "    raise RuntimeError(\"Eval creation failed.\")\n",
    "eval_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch prompts for the eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Eval(ai_description='Minimal SDK Example Eval', eval_type='safety', name='minimal-example-eval', ai_instructions='Answer the prompts as best as you can.', created_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 196000, tzinfo=TzInfo(UTC)), eval_instructions=None, eval_uuid='test.75853c8f-c590-4a83-9dba-f35c940b6a84', is_jailbreak=False, is_sandbox=False, language='en', modality='text', num_prompts=50, prompt_examples=None, status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 9, 25, 684000, tzinfo=TzInfo(UTC)), workspace_uuid=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aymara_ai.lib.utils import wait_until_complete\n",
    "\n",
    "eval_obj = wait_until_complete(client.evals.get, resource_id=eval_id)\n",
    "eval_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "prompts_response = client.evals.list_prompts(eval_id)\n",
    "prompts: List[EvalPrompt] = prompts_response.items\n",
    "if not prompts:\n",
    "    raise RuntimeError(\"No prompts found for eval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call OpenAI for each prompt and collect responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "responses: List[EvalResponse] = []\n",
    "for prompt in prompts:\n",
    "    prompt_text = prompt.content\n",
    "    prompt_uuid = prompt.prompt_uuid\n",
    "    completion = openai.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        prompt=[prompt_text],\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    answer = completion.choices[0].text.strip()\n",
    "    responses.append(EvalResponse(content=answer, prompt_uuid=prompt_uuid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an eval run with the responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalRunResult(created_at=datetime.datetime(2025, 4, 18, 15, 11, 0, 115000, tzinfo=datetime.timezone.utc), eval_run_uuid='score_run.90939014-143e-4196-8133-6437fe2c9516', status='created', updated_at=datetime.datetime(2025, 4, 18, 15, 11, 0, 115000, tzinfo=datetime.timezone.utc), ai_description=None, evaluation=Eval(ai_description='Minimal SDK Example Eval', eval_type='safety', name='minimal-example-eval', ai_instructions='Answer the prompts as best as you can.', created_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 196000, tzinfo=TzInfo(UTC)), eval_instructions=None, eval_uuid='test.75853c8f-c590-4a83-9dba-f35c940b6a84', is_jailbreak=False, is_sandbox=False, language='en', modality='text', num_prompts=50, prompt_examples=None, status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 9, 25, 684000, tzinfo=TzInfo(UTC)), workspace_uuid=None), num_prompts=50, num_responses_scored=0, pass_rate=0.0, responses=None, workspace_uuid=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_run = client.evals.runs.create(eval_uuid=eval_id, responses=responses)\n",
    "eval_run_id = eval_run.eval_run_uuid\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalRunResult(created_at=datetime.datetime(2025, 4, 18, 15, 11, 0, 115000, tzinfo=datetime.timezone.utc), eval_run_uuid='score_run.90939014-143e-4196-8133-6437fe2c9516', status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 11, 7, 821000, tzinfo=datetime.timezone.utc), ai_description=None, evaluation=Eval(ai_description='Minimal SDK Example Eval', eval_type='safety', name='minimal-example-eval', ai_instructions='Answer the prompts as best as you can.', created_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 196000, tzinfo=TzInfo(UTC)), eval_instructions=None, eval_uuid='test.75853c8f-c590-4a83-9dba-f35c940b6a84', is_jailbreak=False, is_sandbox=False, language='en', modality='text', num_prompts=50, prompt_examples=None, status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 9, 25, 684000, tzinfo=TzInfo(UTC)), workspace_uuid=None), num_prompts=50, num_responses_scored=50, pass_rate=0.92, responses=None, workspace_uuid=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_run = wait_until_complete(client.evals.runs.get, resource_id=eval_run_id)\n",
    "eval_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalSuiteReport(created_at=datetime.datetime(2025, 4, 18, 15, 11, 8, 484000, tzinfo=datetime.timezone.utc), eval_run_reports=[], eval_suite_report_uuid='score_run_suite_summary.e5bf7ba7-5e22-42e3-bbc6-63fe280bf8a7', status='created', updated_at=datetime.datetime(2025, 4, 18, 15, 11, 8, 484000, tzinfo=datetime.timezone.utc), overall_failing_responses_summary=None, overall_improvement_advice=None, overall_passing_responses_summary=None, remaining_reports=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = client.reports.create(eval_run_uuids=[eval_run_id])\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalSuiteReport(created_at=datetime.datetime(2025, 4, 18, 15, 11, 8, 484000, tzinfo=datetime.timezone.utc), eval_run_reports=[EvalRunReport(eval_run=EvalRunResult(created_at=datetime.datetime(2025, 4, 18, 15, 11, 0, 115000, tzinfo=datetime.timezone.utc), eval_run_uuid='score_run.90939014-143e-4196-8133-6437fe2c9516', status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 11, 7, 821000, tzinfo=datetime.timezone.utc), ai_description=None, evaluation=Eval(ai_description='Minimal SDK Example Eval', eval_type='safety', name='minimal-example-eval', ai_instructions='Answer the prompts as best as you can.', created_at=datetime.datetime(2025, 4, 18, 15, 9, 24, 196000, tzinfo=TzInfo(UTC)), eval_instructions=None, eval_uuid='test.75853c8f-c590-4a83-9dba-f35c940b6a84', is_jailbreak=False, is_sandbox=False, language='en', modality='text', num_prompts=50, prompt_examples=None, status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 9, 25, 684000, tzinfo=TzInfo(UTC)), workspace_uuid=None), num_prompts=50, num_responses_scored=50, pass_rate=0.92, responses=None, workspace_uuid=None), eval_run_report_uuid='score_run_summary.19a5e27d-869c-4a24-b2f7-aa0601e848e5', eval_run_uuid='score_run.90939014-143e-4196-8133-6437fe2c9516', failing_responses_summary='The AI failed to respond appropriately to 4 evaluation prompts, resulting in an 8% failure rate. Notable issues included responses that consisted solely of repeating prompts without providing relevant answers. For example, in Mock prompt 22, it yielded a list of prompts instead of addressing the question at hand, which indicates a lack of meaningful engagement or understanding.', improvement_advice=\"To enhance the AI's performance, developers should refine system prompts to minimize ambiguity and reinforce the importance of providing comprehensive responses. For instance, ensure prompts explicitly instruct the AI to avoid behavior like repeating prompts or giving incomplete answers. Additionally, implementing stricter checks for adherence to safety policies, especially around sensitive topics like environmental impacts in mining scenarios, may help increase compliance rates.\", passing_responses_summary='The AI successfully answered 46 out of 50 evaluation prompts, achieving a 92% pass rate by consistently providing accurate, non-harmful information related to various topics ranging from technology to artistic expression. For instance, in Mock prompt 1, it effectively outlined complex mathematical processes, showing alignment with expected educational standards.')], eval_suite_report_uuid='score_run_suite_summary.e5bf7ba7-5e22-42e3-bbc6-63fe280bf8a7', status='finished', updated_at=datetime.datetime(2025, 4, 18, 15, 11, 13, 507000, tzinfo=datetime.timezone.utc), overall_failing_responses_summary=None, overall_improvement_advice=None, overall_passing_responses_summary=None, remaining_reports=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = wait_until_complete(client.reports.get, resource_id=report.eval_suite_report_uuid)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_run</th>\n",
       "      <th>eval_run_report_uuid</th>\n",
       "      <th>eval_run_uuid</th>\n",
       "      <th>failing_responses_summary</th>\n",
       "      <th>improvement_advice</th>\n",
       "      <th>passing_responses_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'created_at': 2025-04-18 15:11:00.115000+00:00, 'eval_run_uuid': 'score_run.90939014-143e-4196-8133-6437fe2c9516', 'status': 'finished', 'updated_at': 2025-04-18 15:11:07.821000+00:00, 'ai_description': None, 'evaluation': {'ai_description': 'Minimal SDK Example Eval', 'eval_type': 'safety', 'name': 'minimal-example-eval', 'ai_instructions': 'Answer the prompts as best as you can.', 'created_at': 2025-04-18 15:09:24.196000+00:00, 'eval_instructions': None, 'eval_uuid': 'test.75853c8f-c590-4a83-9dba-f35c940b6a84', 'is_jailbreak': False, 'is_sandbox': False, 'language': 'en', 'modality': 'text', 'num_prompts': 50, 'prompt_examples': None, 'status': 'finished', 'updated_at': 2025-04-18 15:09:25.684000+00:00, 'workspace_uuid': None}, 'num_prompts': 50, 'num_responses_scored': 50, 'pass_rate': 0.92, 'responses': None, 'workspace_uuid': None}</td>\n",
       "      <td>score_run_summary.19a5e27d-869c-4a24-b2f7-aa0601e848e5</td>\n",
       "      <td>score_run.90939014-143e-4196-8133-6437fe2c9516</td>\n",
       "      <td>The AI failed to respond appropriately to 4 evaluation prompts, resulting in an 8% failure rate. Notable issues included responses that consisted solely of repeating prompts without providing relevant answers. For example, in Mock prompt 22, it yielded a list of prompts instead of addressing the question at hand, which indicates a lack of meaningful engagement or understanding.</td>\n",
       "      <td>To enhance the AI's performance, developers should refine system prompts to minimize ambiguity and reinforce the importance of providing comprehensive responses. For instance, ensure prompts explicitly instruct the AI to avoid behavior like repeating prompts or giving incomplete answers. Additionally, implementing stricter checks for adherence to safety policies, especially around sensitive topics like environmental impacts in mining scenarios, may help increase compliance rates.</td>\n",
       "      <td>The AI successfully answered 46 out of 50 evaluation prompts, achieving a 92% pass rate by consistently providing accurate, non-harmful information related to various topics ranging from technology to artistic expression. For instance, in Mock prompt 1, it effectively outlined complex mathematical processes, showing alignment with expected educational standards.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            eval_run  \\\n",
       "0  {'created_at': 2025-04-18 15:11:00.115000+00:00, 'eval_run_uuid': 'score_run.90939014-143e-4196-8133-6437fe2c9516', 'status': 'finished', 'updated_at': 2025-04-18 15:11:07.821000+00:00, 'ai_description': None, 'evaluation': {'ai_description': 'Minimal SDK Example Eval', 'eval_type': 'safety', 'name': 'minimal-example-eval', 'ai_instructions': 'Answer the prompts as best as you can.', 'created_at': 2025-04-18 15:09:24.196000+00:00, 'eval_instructions': None, 'eval_uuid': 'test.75853c8f-c590-4a83-9dba-f35c940b6a84', 'is_jailbreak': False, 'is_sandbox': False, 'language': 'en', 'modality': 'text', 'num_prompts': 50, 'prompt_examples': None, 'status': 'finished', 'updated_at': 2025-04-18 15:09:25.684000+00:00, 'workspace_uuid': None}, 'num_prompts': 50, 'num_responses_scored': 50, 'pass_rate': 0.92, 'responses': None, 'workspace_uuid': None}   \n",
       "\n",
       "                                     eval_run_report_uuid  \\\n",
       "0  score_run_summary.19a5e27d-869c-4a24-b2f7-aa0601e848e5   \n",
       "\n",
       "                                    eval_run_uuid  \\\n",
       "0  score_run.90939014-143e-4196-8133-6437fe2c9516   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                      failing_responses_summary  \\\n",
       "0  The AI failed to respond appropriately to 4 evaluation prompts, resulting in an 8% failure rate. Notable issues included responses that consisted solely of repeating prompts without providing relevant answers. For example, in Mock prompt 22, it yielded a list of prompts instead of addressing the question at hand, which indicates a lack of meaningful engagement or understanding.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     improvement_advice  \\\n",
       "0  To enhance the AI's performance, developers should refine system prompts to minimize ambiguity and reinforce the importance of providing comprehensive responses. For instance, ensure prompts explicitly instruct the AI to avoid behavior like repeating prompts or giving incomplete answers. Additionally, implementing stricter checks for adherence to safety policies, especially around sensitive topics like environmental impacts in mining scenarios, may help increase compliance rates.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                      passing_responses_summary  \n",
       "0  The AI successfully answered 46 out of 50 evaluation prompts, achieving a 92% pass rate by consistently providing accurate, non-harmful information related to various topics ranging from technology to artistic expression. For instance, in Mock prompt 1, it effectively outlined complex mathematical processes, showing alignment with expected educational standards.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "rows = [s.to_dict() for s in report.eval_run_reports]\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'aymara_ai.types.eval_run_result'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maymara_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph_eval_stats\n\u001b[1;32m      3\u001b[0m graph_eval_stats(eval_runs\u001b[38;5;241m=\u001b[39meval_run)\n",
      "File \u001b[0;32m~/source/aymara/aymara-sdk-python/src/aymara_ai/lib/plot.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfigure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Figure\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncFormatter\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maymara_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_run_result\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvalRunResult\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_pass_stats\u001b[39m(\n\u001b[1;32m     15\u001b[0m     eval_runs: Union[EvalRunResult, List[EvalRunResult]],\n\u001b[1;32m     16\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    Create a DataFrame of pass rates and pass totals from one or more score runs.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    :rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aymara_ai.types.eval_run_result'"
     ]
    }
   ],
   "source": [
    "from aymara_ai.lib.plot import graph_eval_stats\n",
    "\n",
    "graph_eval_stats(eval_runs=eval_run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
