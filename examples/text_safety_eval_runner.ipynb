{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AymaraAI Text-to-Text Safety Eval with EvalRunner and AsyncEvalRunner\n",
    "\n",
    "This notebook demonstrates how to use both the synchronous `EvalRunner` and asynchronous `AsyncEvalRunner` for text-to-text safety evaluation with the AymaraAI SDK, using the updated callable interface (callable takes only a prompt string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Set `OPENAI_API_KEY` and `AYMARA_AI_API_KEY` in your environment or `.env` file.\n",
    "- Install dependencies:\n",
    "  ```bash\n",
    "  pip install openai aymara-ai dotenv\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and imports\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from aymara_ai import AymaraAI, AsyncAymaraAI\n",
    "from aymara_ai.lib.df import to_scores_df\n",
    "from aymara_ai.lib.runner import EvalRunner, AsyncEvalRunner\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Callables (Updated Interface)\n",
    "\n",
    "The callable interface now takes only a single argument: the prompt string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronous model callable for EvalRunner\n",
    "def openai_model_callable(prompt: str) -> str:\n",
    "    completion = openai.completions.create(\n",
    "        model=\"gpt-4.1-nano-2025-04-14\",\n",
    "        prompt=[prompt],\n",
    "        max_tokens=256,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return completion.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous model callable for AsyncEvalRunner\n",
    "async def openai_model_callable_async(prompt: str) -> str:\n",
    "    # The OpenAI Python SDK is not natively async, so we run the sync call in a thread executor\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, lambda: openai_model_callable(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Eval Parameters\n",
    "\n",
    "We will use a basic text-to-text safety eval configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_description = \"A helpful AI assistant.\"\n",
    "ai_instructions = \"Please provide detailed answers to the prompts.\"\n",
    "\n",
    "eval_params = {\n",
    "    \"ai_description\": ai_description,\n",
    "    \"ai_instructions\": ai_instructions,\n",
    "    \"eval_type\": \"safety\",\n",
    "    \"name\": \"text-to-text safety eval (runner example)\",\n",
    "    \"num_prompts\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Evaluation with EvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AymaraAI()\n",
    "runner = EvalRunner(client, openai_model_callable)\n",
    "eval_run = runner.run_eval(eval_params)\n",
    "display(f\"Eval Run ID: {eval_run.eval_run_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Evaluation with AsyncEvalRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_async_eval():\n",
    "    async_client = AsyncAymaraAI()\n",
    "    runner = AsyncEvalRunner(async_client, openai_model_callable_async)\n",
    "    eval_run = await runner.run_eval(eval_params)\n",
    "    return eval_run\n",
    "\n",
    "eval_run_async = asyncio.run(run_async_eval())\n",
    "display(f\"Async Eval Run ID: {eval_run_async.eval_run_uuid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for synchronous run\n",
    "prompts = client.evals.list_prompts(runner.eval_id).items\n",
    "responses = client.evals.runs.list_responses(runner.run_id).items\n",
    "to_scores_df(eval_run, prompts, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results for async run\n",
    "async_client = AsyncAymaraAI()\n",
    "prompts_async = asyncio.run(async_client.evals.list_prompts(eval_run_async.eval_uuid)).items\n",
    "responses_async = asyncio.run(async_client.evals.runs.list_responses(eval_run_async.eval_run_uuid)).items\n",
    "to_scores_df(eval_run_async, prompts_async, responses_async)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Visualize with graph_eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from aymara_ai.lib.plot import graph_eval_stats  # type: ignore\n",
    "    graph_eval_stats(eval_runs=[eval_run, eval_run_async])\n",
    "except ImportError:\n",
    "    display(\"Plotting utility not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use both the synchronous `EvalRunner` and asynchronous `AsyncEvalRunner` for text-to-text safety evaluation with the AymaraAI SDK, using the updated callable interface. Use the synchronous runner for simple, blocking workflows, and the async runner for scalable or concurrent evaluation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
